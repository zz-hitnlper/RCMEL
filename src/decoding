import torch
from tqdm import tqdm
import ujson
import torch.multiprocessing as mp
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, AutoModel, LlavaNextProcessor, LlavaNextForConditionalGeneration
from util import *
import re
# 根据提及的20个最相关构建实体表格
def get_entity_table_diff(mention_info,entity_dict,m):
    # Initialize the output string
    output = ""
    # Counter for entity numbering
    entity_count = 1
    # Iterate through the top 20 IDs
    for qid in mention_info['ndtop10'][:30]:  # Ensure we only take 20
        if qid in entity_dict:
            entity = entity_dict[qid]
            # output += f"-qid：{entity['qid']}\n"
            output += str(entity_count)+". "+entity['entity_name']+": "+entity['desc_image_1sentence']+" "+mention_info["knowledge_contrast"][entity_count-1]+"\n"
            # output += f"-cotegary：{entity['instance']}\n"
            # output += f"-description：{entity['desc_image_1sentence']}\n"
        else:
            # If the entity is not found in the provided data, add a placeholder
            # output += f"Entity {entity_count}：\n"
            # output += f"-qid：{qid}\n"
            # output += f"-name：Unknown\n"
            # output += f"-cotegary：Unknown\n"
            # output += f"-description：Entity data not provided for this qid.\n"
            pass
        entity_count += 1

    # Print the result
    return output


def subprocess_decoding(gpu_id, args, mention_chunk, qid_entity_dict, mention_output_dir):
    # 设置当前进程的 GPU
    torch.cuda.set_device(gpu_id)

    # 加载 LLM
    tokenizer = AutoTokenizer.from_pretrained(args.llm)
    model = AutoModelForCausalLM.from_pretrained(
        args.llm, device_map=f"cuda:{gpu_id}", torch_dtype=torch.bfloat16
    )
    llmpipeline = pipeline(
        "text-generation", model=model, tokenizer=tokenizer, device_map=f"cuda:{gpu_id}"
    )

    # 提示模板
    system = "you are a helpful assistant!"
    PROMPT3_ranker=PROMPTS['PROMPT3_ranker']


    result=[]
    for i in tqdm(mention_chunk, desc=f"GPU {gpu_id} 排序解码"):
        # 根据候选创建提示内容 entity table
        Entity_table_info = get_entity_table_diff(i, qid_entity_dict,args.decoding.m)# m是候选实体，n是几个结果
        outputprompt=getoutputprompt(args.decoding.m,1)
        

        # 创建提示内容
        text = PROMPT3_ranker.format(Entity_table_info=Entity_table_info,mention_name=i['mentions'],mention_context=i['sentence'],mention_des=i['desc_image_1sentence'],mention_cate=qid_entity_dict[i['answer']]['instance'],OutPut=outputprompt)
        # text = PROMPT3_knowledge_contrast.format(Entity_table_info=Entity_table_info)
        # text="Please describe how Entity 1 differs from the other entities based on the following information."+Entity_table_info
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": text},
        ]
        prompt = llmpipeline.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        terminators = [
            llmpipeline.tokenizer.eos_token_id,
            llmpipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        try:
            outputs = llmpipeline(
                prompt,
                max_new_tokens=10,
                eos_token_id=terminators,
                do_sample=True,
                temperature=0.6,
                top_p=0.9,
                pad_token_id=128001
            )
            output = outputs[0]["generated_text"][len(prompt):]
            result.append({
                "id":i['id'],
                "mentions":i['mentions'],
                "entities":i['entities'],
                "answer":i['answer'],
                "true":i['ndtop10'].index(i['answer'])+1,
                "top1_30":output
            })
        except Exception as e:
            print(i['id'])
            torch.cuda.empty_cache()  # 清理显存缓存

    # 保存子集结果
    subset_output = f"{mention_output_dir}_gpu{gpu_id}.json"
    with open(subset_output, "w", encoding="utf-8") as f:
        ujson.dump(result, f, indent=4, ensure_ascii=False)
    return subset_output
   

def infer_decoding_multigpu(args):
    assert torch.cuda.device_count() >= 3, f"Requires at least 3 GPUs, but only {torch.cuda.device_count()} available."

    # Load entity data
    entity_dir = args.code_path + args.infer_kc.entity_desc_llm_mllm_dir
    with open(entity_dir, 'r', encoding="utf-8") as f:
        entity_data = ujson.load(f)
    qid_entity_dict={i['qid']:i for i in entity_data}
    # Load mention data
    mention_data_dir = args.code_path + args.decoding.mention_entity_diff_dir
    mention_output_dir = args.code_path + args.decoding.result    
    with open(mention_data_dir, "r") as f:
        mention_data = ujson.load(f)

    # Split mention_data into 3 chunks
    chunk_size = len(mention_data) // 3
    mention_chunks = [mention_data[i * chunk_size:(i + 1) * chunk_size] for i in range(3)]
    if len(mention_data) % 3 != 0:
        mention_chunks[-1].extend(mention_data[3 * chunk_size:])

    # Parallel processing on 3 GPUs
    processes = []
    for gpu_id in range(3):
        p = mp.Process(target=subprocess_decoding, args=(gpu_id, args, mention_chunks[gpu_id], qid_entity_dict, mention_output_dir))
        processes.append(p)
        p.start()
    for p in processes:
        p.join()

    # Combine results
    combined_data = []
    for gpu_id in range(3):
        with open(f"{mention_output_dir}_gpu{gpu_id}.json", "r", encoding="utf-8") as f:
            combined_data.extend(ujson.load(f))
    with open(mention_output_dir, "w", encoding="utf-8") as f:
        ujson.dump(combined_data, f, indent=4, ensure_ascii=False)
